{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65ee5ef2-8ad0-44f8-86de-a86887b020d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === WHAT YOU'LL NEED TO INTERACT WITH ===\n",
    "\n",
    "# download_dir\n",
    "# This defines the download location. The default, \"/Users/ashlanjo/Downloads/Statcom_final\", should be updated to a path\n",
    "# appropriate for your system. It's recommended to create a dedicated folder for these downloads.\n",
    "# The code will redownload files to this folder if they are stored elsewhere or don't follow the naming convention.\n",
    "# To preserve old files, copy them into the new folder (rather than moving or renaming them).\n",
    "\n",
    "# last_scrap_date\n",
    "# This marks the last date the script is assumed to have been run. Files dated before this are skipped, saving significant time.\n",
    "# It's recommended to backdate last_scrap_date by ~2 days to avoid missing late-uploaded files (e.g., uploads made later the same day).\n",
    "# Files already downloaded won't be downloaded again, so this buffer is harmless.\n",
    "# To force downloading older files, set last_scrap_date to a date far in the past (e.g., 20 years ago).\n",
    "# This is easier than modifying the logic that skips older files.\n",
    "\n",
    "# urls_to_run = sub_urls\n",
    "# This should generally be left unchanged. However, to run the code for specific agencies, you can redefine sub_urls to a subset.\n",
    "# Use print(urls_to_run) and check the links to ensure the correct agencies are selected.\n",
    "# The order in sub_urls matches the order on the website.\n",
    "# Be sure to reset it back to the full list once you're done.\n",
    "\n",
    "# Manual interruptions and errors\n",
    "# If you manually stop the code, make sure to run driver.quit() before doing anything else.\n",
    "# This ensures that the browser is properly closed so it can reopen for future scraping sessions.\n",
    "# If the code errors out, driver.quit() is automatically executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2b8d6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "chrome_server = Service(executable_path='/Users/stefaneng/lib/chromedriver-mac-arm64/chromedriver')\n",
    "chrome_server.start()\n",
    "driver = webdriver.Chrome(service=chrome_server)\n",
    "driver.get('http://www.google.com/')\n",
    "time.sleep(5) # Let the user actually see something!\n",
    "search_box = driver.find_element(\"name\", \"q\")\n",
    "search_box.send_keys('ChromeDriver')\n",
    "search_box.submit()\n",
    "time.sleep(5) # Let the user actually see something!\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466ef61b-5010-490e-a1cf-2e6c9b818ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To Do\n",
    "#Send to Kevin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dbcee9c-23b4-4a66-bbb7-970e687bf2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selenium is used for web automation and scraping\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import ElementClickInterceptedException\n",
    "# Time utilities for delays and timestamp handling\n",
    "import time\n",
    "from time import sleep\n",
    "# Date and time parsing\n",
    "from datetime import datetime\n",
    "# File and path operations\n",
    "from pathlib import Path\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a15b4497-a156-440b-8105-22eb3ab9abf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more pages available.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "284"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This code collects the links for all of the individual agencies (location where children stay)\n",
    "#These links will be iterated through in future code chunks\n",
    "\n",
    "# Initialize a Chrome browser session and navigate to the main licensing search page\n",
    "chrome_server = Service(executable_path='/Users/stefaneng/lib/chromedriver-mac-arm64/chromedriver')\n",
    "chrome_server.start()\n",
    "driver = webdriver.Chrome(service=chrome_server)\n",
    "\n",
    "try:\n",
    "    driver.get(\"https://michildwelfarepubliclicensingsearch.michigan.gov/licagencysrch/\")\n",
    "    time.sleep(20) # Wait for the page to fully load (longer than usual due to dynamic content)\n",
    "    # Prepare an empty list to store all agency-specific URLs found across pages\n",
    "\n",
    "    sub_urls = []\n",
    "\n",
    "    while True:\n",
    "        # Locate all agency links on the current page using their XPATH\n",
    "        agency_urls = driver.find_elements(By.XPATH, \"//td/lightning-primitive-cell-factory/span/div/lightning-formatted-url/a\")\n",
    "\n",
    "        for agency_url in agency_urls:\n",
    "            # Extract the actual hyperlink from each agency link element\n",
    "            sub_url = agency_url.get_attribute('href')\n",
    "            sub_urls.append(sub_url) # Add it to the growing list of sub-URLs\n",
    "\n",
    "        try:\n",
    "            # Try to locate and click the \"Next\" page button to load the next page of results\n",
    "            next_button = driver.find_element(By.XPATH, \"//lightning-button-icon[3]/button/lightning-primitive-icon\")\n",
    "            next_button.click()\n",
    "        except ElementClickInterceptedException:\n",
    "            # If the click fails (e.g., no more pages or overlay blocking it), stop the loop\n",
    "            print(\"No more pages available.\")\n",
    "            break\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n",
    "# Show the number of agency URLs collected\n",
    "len(sub_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdebc7e-5d93-48a2-85b3-7dad47ebd038",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All files prior to this date will not be downloaded\n",
    "#All files on or after this data will usually be downloaded\n",
    "#The exception is if the file is already in the download location with the correct name\n",
    "last_scrap_date = \"7-01-2025\"\n",
    "last_scrap_date = datetime.strptime(last_scrap_date, '%m-%d-%Y').date()\n",
    "\n",
    "#All files will be downloaded here\n",
    "download_dir = \"/Users/stefaneng/Library/CloudStorage/Dropbox-UniversityofMichigan/Stefan Eng/MCYJ_parsing/test_download\"\n",
    "\n",
    "#This line can be adjusted to run only certain agencies\n",
    "urls_to_run = sub_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c04ac57-7939-4079-90cd-c7582ac0c782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glen's_House\n",
      "Number of rows found: 4\n",
      "Glens_House_interim_2024  and all files before it should have been downloaded before\n",
      "Mission_Ranch\n",
      "Number of rows found: 11\n",
      "Mission_Ranch_Renewal_2025  and all files before it should have been downloaded before\n",
      "Heartland_Center_for_Autism\n",
      "Number of rows found: 6\n",
      "2025SIC0000409  and all files before it should have been downloaded before\n",
      "Osceola_Youth_Center\n",
      "Number of rows found: 25\n",
      "Clicked on element in row with document '2025SIC0000657'\n",
      "Renamed file to: Osceola_Youth_Center_2025SIC0000657_2025-07-09.pdf\n",
      "Clicked on element in row with document '2025SIC000737'\n",
      "Renamed file to: Osceola_Youth_Center_2025SIC000737_2025-07-09.pdf\n",
      "Clicked on element in row with document '2025SIC0000763'\n",
      "Renamed file to: Osceola_Youth_Center_2025SIC0000763_2025-07-02.pdf\n",
      "2025SIC0000655  and all files before it should have been downloaded before\n",
      "Let's_Talk_About_it_Girl's_Home_III\n",
      "Number of rows found: 17\n",
      "2025SIC0000512  and all files before it should have been downloaded before\n",
      "House_of_Love_Agency\n",
      "Number of rows found: 16\n",
      "House_of_Love_2024_Renewal_inspection_AMENDED  and all files before it should have been downloaded before\n",
      "Jackson_House\n",
      "Number of rows found: 3\n",
      "Jackson_House_2025_Interim_Inspection_(1)  and all files before it should have been downloaded before\n",
      "VISTA_MARIA\n",
      "Number of rows found: 50\n",
      "2025SIC0000694  and all files before it should have been downloaded before\n",
      "New_Hope_Youth_Center\n",
      "Number of rows found: 41\n",
      "2025SIC0000681  and all files before it should have been downloaded before\n",
      "EAGLE_VILLAGE_LEPPIEN\n",
      "Number of rows found: 18\n",
      "2025SIC0000333  and all files before it should have been downloaded before\n",
      "PINE_WAY_GROUP_SOUTH\n",
      "Number of rows found: 12\n",
      "Pine_Way_South_interim_2025  and all files before it should have been downloaded before\n",
      "LAKES_AREA_TEACH_FAMILY_HOME\n",
      "Number of rows found: 12\n",
      "Lakes_Area_Teaching_Family_Homes_Renewal_2025  and all files before it should have been downloaded before\n",
      "PENRICKTON_CENTER_FOR_BLIND_CHILDREN\n",
      "Number of rows found: 8\n",
      "2025_Interim_Report  and all files before it should have been downloaded before\n",
      "Children's_Village\n",
      "Number of rows found: 19\n",
      "2025SIC0000448  and all files before it should have been downloaded before\n",
      "PECKHAM,_INC._-_FOOTPRINTS\n",
      "Number of rows found: 7\n",
      "Peckham_Inc._Footprints_Interim_2024  and all files before it should have been downloaded before\n",
      "BETHANY_CHRISTIAN_SERVICES_INC-RESIDENTIAL\n",
      "Number of rows found: 18\n",
      "2025SIC0000577  and all files before it should have been downloaded before\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m urls_to_run:\n\u001b[32m     11\u001b[39m         \u001b[38;5;66;03m# Open the target page\u001b[39;00m\n\u001b[32m     12\u001b[39m     driver.get(url)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     time.sleep(\u001b[32m20\u001b[39m)\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# Get the document agency name\u001b[39;00m\n\u001b[32m     16\u001b[39m     document_agency_element = driver.find_elements(By.XPATH, \u001b[33m\"\u001b[39m\u001b[33m//lightning-layout-item[1]/slot/div[1]/div\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#Initialize the browser\n",
    "chromeOptions = webdriver.ChromeOptions()\n",
    "prefs = {\"download.default_directory\": download_dir}\n",
    "chromeOptions.add_experimental_option(\"prefs\", prefs)\n",
    "#driver = webdriver.Chrome(options=chromeOptions)\n",
    "driver = webdriver.Chrome(service=chrome_server, options=chromeOptions)\n",
    "\n",
    "#Iterates through each of the urls collected above\n",
    "try:\n",
    "    for url in urls_to_run:\n",
    "            # Open the target page\n",
    "        driver.get(url)\n",
    "        time.sleep(20)\n",
    "\n",
    "        # Get the document agency name\n",
    "        document_agency_element = driver.find_elements(By.XPATH, \"//lightning-layout-item[1]/slot/div[1]/div\")\n",
    "        document_agency = \"_\".join([element.text.strip().replace(\" \", \"_\").replace(\"/\", \"_\") for element in document_agency_element]) if document_agency_element else \"Unknown_Agency\"\n",
    "\n",
    "        print(document_agency)\n",
    "\n",
    "        # Locate all rows within the table\n",
    "        table_rows = driver.find_elements(By.XPATH, \"//lightning-datatable/div[2]/div/div/table/tbody/tr\")\n",
    "        print(\"Number of rows found:\", len(table_rows))\n",
    "\n",
    "        # Iterate through each row/ document\n",
    "        for row in reversed(table_rows):\n",
    "            try:\n",
    "                # Extract document date from the 1st column and reformats it\n",
    "                document_date_element = row.find_element(By.XPATH, \"./td[1]\")\n",
    "                document_date = document_date_element.text.strip().replace(\" \", \"_\")\n",
    "                sanitized_date = document_date.replace(\"/\", \"-\")\n",
    "                sanitized_date = datetime.strptime(sanitized_date, '%m-%d-%Y').date()\n",
    "\n",
    "                # Extract document name from the 2nd column and reformats it\n",
    "                document_name_element = row.find_element(By.XPATH, \"./td[2]\")\n",
    "                document_name_temp = document_name_element.text.strip().replace(\" \", \"_\")\n",
    "                document_name = document_name_temp.replace(\"/\", \"-\")\n",
    "\n",
    "                #Skips all the files that should have been scrapped on a prior run\n",
    "                if sanitized_date < last_scrap_date:\n",
    "                    print(document_name, \" and all files before it should have been downloaded before\")\n",
    "                    break\n",
    "\n",
    "                # Construct the new file name\n",
    "                new_file_name = f\"{document_agency}_{document_name}_{sanitized_date}.pdf\"\n",
    "                new_file_path = os.path.join(download_dir, new_file_name)\n",
    "\n",
    "                #If the file is already downloaded (and follows the naming convention)\n",
    "                #It will skip it\n",
    "                if os.path.exists(new_file_path):\n",
    "                    print(document_name, \"has been downloaded since last scrap\")\n",
    "                    continue\n",
    "\n",
    "                # Locate the clickable element in the 4th column\n",
    "                clickable_element = row.find_element(By.XPATH, \"./td[4]\")\n",
    "\n",
    "                # Check if the element is clickable\n",
    "                if clickable_element.is_enabled() and clickable_element.is_displayed():\n",
    "                    # Click to download\n",
    "                    clickable_element.click()\n",
    "                    print(f\"Clicked on element in row with document '{document_name}'\")\n",
    "\n",
    "                    # Wait for the file to download (adjust wait time as needed)\n",
    "                    time.sleep(10)\n",
    "\n",
    "                    # Find the most recently downloaded file\n",
    "                    list_of_files = glob.glob(f\"{download_dir}/*\")\n",
    "                    latest_file = max(list_of_files, key=os.path.getctime)\n",
    "\n",
    "                    # Rename the file\n",
    "                    os.rename(latest_file, new_file_path)\n",
    "                    print(f\"Renamed file to: {new_file_name}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"Failed to click on element in row:\", e)\n",
    "    driver.quit()\n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55ff624-97a8-4df8-a5e7-10b988a90de4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2f8856-b204-45dc-b51f-1ddcf4c6f4e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MCYJ_parsing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
